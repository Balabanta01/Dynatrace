Context/Problem Statement:

"Currently, setting optimal thresholds for response time degradation detection in Dynatrace often relies on manual estimation or generic guidelines. This can lead to either excessive alerting (false positives) or missed critical performance issues (false negatives). Many users struggle with finding the right balance, especially in dynamic environments where application behavior can fluctuate significantly.

Proposed Solution/Enhancement:

"Leveraging real-time data analysis, such as median response time and slowest 10% response time, directly within the Dynatrace configuration can significantly improve the accuracy and effectiveness of automatic baselining. By automatically populating or suggesting threshold values based on these real-time metrics, users can:

Reduce manual configuration effort: Eliminating the need for manual estimation and trial-and-error.
Improve threshold accuracy: Ensuring thresholds are aligned with the application's actual performance characteristics.
Minimize false positives and negatives: Leading to more relevant and actionable alerts.
Adapt to dynamic changes: Automatically adjusting thresholds as application behavior evolves.
Example Scenario (Using the Data from Your Image):

"Consider a service, abc-strapi-web, exhibiting the following performance metrics:

Median Response Time: 7.77 ms
Slowest 10% Response Time: 81.7 ms
"Instead of relying on generic thresholds, Dynatrace could automatically suggest or populate the following values for response time degradation detection:

All Requests:
Absolute Threshold: 50-75 ms (based on a reasonable buffer above the median)
Relative Threshold: 50% (allowing for moderate fluctuations)
Slowest 10%:
Absolute Threshold: 200 ms (significantly higher to account for expected variations)
Relative Threshold: 100% (allowing for larger variations in the slowest requests)
"These suggested values would be based on the real-time data, providing a more accurate and context-aware baseline.

Benefits:

Proactive Performance Management: Identify and address performance issues before they significantly impact users.
Reduced Alert Fatigue: Focus on relevant alerts, improving operational efficiency.
Faster Incident Response: Quickly pinpoint and resolve performance degradations.
Improved User Experience: Maintain consistent and optimal application performance.
Call to Action:

"We propose that Dynatrace engineers consider implementing this feature to enhance automatic baselining for response time degradation detection. This would empower users to leverage real-time data for more effective monitoring and proactive performance management. We welcome feedback and suggestions from the Dynatrace community on this proposal."

Scenario based example: 

!

Data Analysis
Median Response Time: 7.77 ms
Slowest 10% Response Time: 81.7 ms
Failure Rate: 0% (This is good, but we'll focus on response time)
Requests: 21 /min (This gives us an idea of the request volume)
Applying the Best Practices
Based on the data and the best practices we discussed earlier, here's how we can suggest values for the fields:
1. All Requests Thresholds
Absolute Threshold (ms):
Start with a conservative value: Since the median is 7.77 ms, let's start with a value significantly higher but still reasonable.  A value around 50-75 ms would be a good starting point.
Consider user experience: A 50-75 ms delay is generally not noticeable to most users, but it's important to consider the specific application.
Suggestion: 50 ms or 75 ms
Relative Threshold (%):
Start with a moderate percentage: Let's begin with a 50% increase.
Calculate the increase: 7.77 ms * 0.50 = 3.885 ms.  Adding this to the median gives us 11.655 ms.
Suggestion: 50%
2. Slowest 10% Thresholds
Absolute Threshold (ms):
Set a higher value than the median: The slowest 10% is 81.7 ms. Let's start with a value significantly higher, like 200 ms.
Consider outliers: We don't have information on outliers, but 200 ms seems like a reasonable initial value.
Suggestion: 200 ms
Relative Threshold (%):
Start with a higher percentage than the "All requests" threshold: Let's use 100%.
Calculate the increase: 81.7 ms * 1.00 = 81.7 ms. Adding this to the slowest 10% gives us 163.4 ms.
Suggestion: 100%
Summary of Suggested Values
All Requests:
Absolute Threshold: 50 ms or 75 ms
Relative Threshold: 50%
Slowest 10%:
Absolute Threshold: 200 ms
Relative Threshold: 100%
Important Notes:
Iteration is key: These are starting values. You must monitor the alerts generated by your system and adjust the thresholds based on your observations and the specific needs of your application.
Context matters: If this application is very sensitive to latency, you might need to lower the absolute thresholds.
Consider the request volume: With 21 requests/min, the impact of a slow request might be less critical than for a high-volume application.
By using these suggested values and continuously monitoring your system's performance, you can effectively configure automatic baselining for response time degradation detection and ensure a smooth user experience.
 
